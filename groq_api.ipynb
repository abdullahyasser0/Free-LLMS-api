{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmwUOit5PzmpAXnq9eJLnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahyasser0/Free-LLMS-api/blob/main/groq_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî∞ Introduction\n",
        "\n",
        "In this notebook, we will explore how to integrate **groq** into your code, along with usage examples.  \n",
        "To get started:\n",
        "\n",
        "1. Generate a free API key from [groq cloud](https://console.groq.com/keys) (sign in or create an account).\n",
        "2. For security, store your API key in **Colab secrets** instead of directly in the code.\n"
      ],
      "metadata": {
        "id": "Yh8EeUCJFG-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports ‚öì"
      ],
      "metadata": {
        "id": "eTKOer9KjWnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m7LQ-j9fEAhy",
        "outputId": "5900f9dc-f23c-4c31-c8d8-98f48277c633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.28.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.28.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.28.0\n"
          ]
        }
      ],
      "source": [
        "pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "XFcHOb4rElUE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=userdata.get('GROQ_API_KEY'))"
      ],
      "metadata": {
        "id": "ccoJ9MkqF6oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text generation\n",
        "\n"
      ],
      "metadata": {
        "id": "TMYw_homhRl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama-3.3-70b-versatile\"\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2iopajBEVLi",
        "outputId": "6dab60dc-de97-456c-efdc-ffb04f128c38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's technological landscape, and their importance can be understood from several perspectives:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models can process and analyze vast amounts of text data quickly, making them essential for applications that require real-time or near-real-time processing, such as:\n",
            "\t* Sentiment analysis\n",
            "\t* Text classification\n",
            "\t* Language translation\n",
            "\t* Chatbots and virtual assistants\n",
            "2. **Improved User Experience**: Fast language models enable applications to respond rapidly to user input, providing a seamless and interactive experience. This is particularly important for:\n",
            "\t* Voice assistants\n",
            "\t* Messaging platforms\n",
            "\t* Customer service chatbots\n",
            "\t* Online search engines\n",
            "3. **Scalability**: Fast language models can handle large volumes of data and user requests, making them scalable for:\n",
            "\t* Big data analytics\n",
            "\t* Natural Language Processing (NLP) tasks\n",
            "\t* Machine learning applications\n",
            "\t* Cloud-based services\n",
            "4. **Energy Efficiency**: Faster language models can lead to significant reductions in computational resources and energy consumption, which is essential for:\n",
            "\t* Edge devices (e.g., smartphones, smart home devices)\n",
            "\t* Data centers\n",
            "\t* Cloud computing\n",
            "\t* Environmental sustainability\n",
            "5. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive edge by:\n",
            "\t* Responding quickly to customer inquiries\n",
            "\t* Analyzing market trends and sentiments in real-time\n",
            "\t* Developing more efficient and effective NLP-based applications\n",
            "\t* Improving overall customer satisfaction and loyalty\n",
            "6. **Research and Development**: Fast language models can accelerate research in areas like:\n",
            "\t* NLP\n",
            "\t* Machine learning\n",
            "\t* Artificial intelligence (AI)\n",
            "\t* Cognitive computing\n",
            "\t* Human-computer interaction\n",
            "7. **Accessibility**: Fast language models can enable more people to access information and services, particularly in:\n",
            "\t* Low-resource environments (e.g., areas with limited internet connectivity)\n",
            "\t* Accessibility applications (e.g., text-to-speech, speech-to-text)\n",
            "\n",
            "In summary, fast language models are vital for efficient processing, improved user experience, scalability, energy efficiency, competitive advantage, research and development, and accessibility. As the demand for NLP-based applications continues to grow, the importance of fast language models will only continue to increase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to Speech\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Converts the plain text to an audio."
      ],
      "metadata": {
        "id": "46SMqqAhF2qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the first time using it you must accept the terms find the link below, or it will appear as an error click and accept\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Model Terms Acceptance\n",
        "To use PlayAI TTS, you must accept the model terms: [link](https://console.groq.com/playground?model=playai-tts)"
      ],
      "metadata": {
        "id": "KrTO7Ds4GNkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "# English Model\n",
        "\n",
        "speech_file_path = \"speech.wav\"\n",
        "model = \"playai-tts\" # \"playai-tts-arabic\" this model supports arabic\n",
        "voice = \"Fritz-PlayAI\"\n",
        "text = \"I love building and shipping new features for our users!\"\n",
        "response_format = \"wav\"\n",
        "\n",
        "response = client.audio.speech.create(\n",
        "    model=model,\n",
        "    voice=voice,\n",
        "    input=text,\n",
        "    response_format=response_format\n",
        ")\n",
        "\n",
        "response.write_to_file(speech_file_path)"
      ],
      "metadata": {
        "id": "wg0KAJpaF3OL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arabic Model\n",
        "\n",
        "speech_file_path = \"speechAraic.wav\"\n",
        "model = \"playai-tts-arabic\" # \"playai-tts-arabic\" this model supports arabic\n",
        "voice = \"Nasser-PlayAI\" # use one of the following \"Nasser-PlayAI\" or \"Khalid-PlayAI\" or \"Amira-PlayAI\" or \"Ahmad-PlayAI\"\n",
        "text = \"ÿßŸÜÿß ÿßÿ≠ÿ® ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"\n",
        "response_format = \"wav\"\n",
        "\n",
        "response = client.audio.speech.create(\n",
        "    model=model,\n",
        "    voice=voice,\n",
        "    input=text,\n",
        "    response_format=response_format\n",
        ")\n",
        "\n",
        "response.write_to_file(speech_file_path)"
      ],
      "metadata": {
        "id": "bjS0RxB8HEuf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech to Text\n",
        "\n",
        "---\n",
        "\n",
        "converts WAV files to text, supports only English"
      ],
      "metadata": {
        "id": "qjCzu6d_Ib7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "\n",
        "\n",
        "# Specify the path to the audio file\n",
        "filename = \"/content/speech.wav\" # Replace with your audio file!\n",
        "\n",
        "# Open the audio file\n",
        "with open(filename, \"rb\") as file:\n",
        "    # Create a transcription of the audio file\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "      file=file, # Required audio file\n",
        "      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
        "      prompt=\"Specify context or spelling\",  # Optional\n",
        "      response_format=\"verbose_json\",  # Optional\n",
        "      timestamp_granularities = [\"word\", \"segment\"], # Optional (must set response_format to \"json\" to use and can specify \"word\", \"segment\" (default), or both)\n",
        "      language=\"en\",  # Optional\n",
        "      temperature=0.0  # Optional\n",
        "    )\n",
        "    # To print only the transcription text, you'd use print(transcription.text) (here we're printing the entire transcription object to access timestamps)\n",
        "    print(json.dumps(transcription, indent=2, default=str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo5smQUbIcXr",
        "outputId": "5c525edf-0229-4b4d-adf9-cd7952e21b01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Transcription(text=' I love building and shipping new features for our users', task='transcribe', language='English', duration=3.1, words=[{'word': 'I', 'start': 0, 'end': 0.16}, {'word': 'love', 'start': 0.16, 'end': 0.36}, {'word': 'building', 'start': 0.36, 'end': 0.68}, {'word': 'and', 'start': 0.68, 'end': 0.86}, {'word': 'shipping', 'start': 0.86, 'end': 1.1}, {'word': 'new', 'start': 1.1, 'end': 1.32}, {'word': 'features', 'start': 1.32, 'end': 1.72}, {'word': 'for', 'start': 1.72, 'end': 1.94}, {'word': 'our', 'start': 1.94, 'end': 2.12}, {'word': 'users', 'start': 2.12, 'end': 2.5}], segments=[{'id': 0, 'seek': 0, 'start': 0, 'end': 3.1, 'text': ' I love building and shipping new features for our users', 'tokens': [50365, 286, 959, 2390, 293, 14122, 777, 4122, 337, 527, 5022, 50520], 'temperature': 0, 'avg_logprob': -0.0911512, 'compression_ratio': 0.9322034, 'no_speech_prob': 4.125372e-12}], x_groq={'id': 'req_01jyd246a6eybt02bpdnxmw24z'})\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Images and Vision\n",
        "\n",
        "---\n",
        "\n",
        "It descripe the given image and the image can be a local file\n"
      ],
      "metadata": {
        "id": "eiCJqpy_JT4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\", #Also supports \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        "\n",
        "\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"What's in this image?\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8PerCzCJUOB",
        "outputId": "5185a7fa-bebf-4f87-9309-c3d975593a68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='The image presents a detailed view of a microchip, showcasing its intricate design and layout. The microchip is composed of various components, including:\\n\\n* **Grid-like patterns**: The majority of the microchip features grid-like patterns, which are likely used for routing and connecting different parts of the chip.\\n* **Rectangular sections**: Two large rectangular sections are visible on either side of the microchip, each comprising a grid of small squares. These sections may represent memory or processing units.\\n* **Decorative borders**: The edges of the microchip feature decorative borders, adding an aesthetic touch to the design.\\n* **Smaller components**: At the bottom of the microchip, several smaller components are visible, which could be used for power management, input/output operations, or other supporting functions.\\n\\nThe overall design of the microchip suggests a high level of complexity and sophistication, with multiple layers and components working together to enable advanced computing capabilities. The use of grid-like patterns and rectangular sections indicates a focus on efficiency and organization, while the decorative borders add a touch of elegance to the design. \\n\\nThe microchip appears to be a central processing unit (CPU) or a similar type of integrated circuit, given its complex layout and design.', role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local image"
      ],
      "metadata": {
        "id": "O8m6mM43KMEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"sf.jpg\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Yln1F1lHKKDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning\n",
        "\n",
        "---\n",
        "\n",
        "Reasoning models excel at complex problem-solving tasks that require step-by-step analysis, logical deduction, and structured thinking and solution validation. With Groq inference speed, these types of models can deliver instant reasoning capabilities critical for real-time applications."
      ],
      "metadata": {
        "id": "if7feo1rKRUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"deepseek-r1-distill-llama-70b\", # \"qwen/qwen3-32b\" , \"qwen-qwq-32b\" , \"deepseek-r1-distill-llama-70b\"\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How many r's are in the word strawberry?\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.6,\n",
        "    max_completion_tokens=1024,\n",
        "    top_p=0.95,\n",
        "    stream=True,\n",
        "    reasoning_format=\"raw\"\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlW6yZpHKSbd",
        "outputId": "0e92c370-00cf-425a-f7f0-2e798ce07ea5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, let's see. The user is asking how many times the letter 'r' appears in the word \"strawberry\". Hmm, first, I need to make sure I have the correct spelling of the word. Let me write it out: S-T-R-A-W-B-E-R-R-Y. Wait, is that right? Let me check again. S-T-R-A-W-B-E-R-R-Y. Yeah, that's the correct spelling. So, breaking it down letter by letter.\n",
            "\n",
            "Starting with the first letter: S. No 'r' there. Next is T, still no. Then R. That's the first 'r'. Then A, no. W, no. B, no. E, no. Then R, that's the second 'r'. And another R right after, so that's the third 'r'. Then Y. So in total, there are three 'r's in \"strawberry\". Wait, let me count again to be sure. S-T-R-A-W-B-E-R-R-Y. The 'r's are in the third position, then after the 'e', there are two 'r's. So positions 3, 8, and 9. That's three 'r's. Hmm, I think that's correct. But sometimes people might miss one if they're not careful. Let me go through each letter one by one again.\n",
            "\n",
            "1. S\n",
            "2. T\n",
            "3. R (1)\n",
            "4. A\n",
            "5. W\n",
            "6. B\n",
            "7. E\n",
            "8. R (2)\n",
            "9. R (3)\n",
            "10. Y\n",
            "\n",
            "Yes, three 'r's. I think that's right. Maybe some people might think there's only two because they overlook the second 'r' after the first one. But no, in \"strawberry\", after the 'e', there are two 'r's. So the answer should be three.\n",
            "</think>\n",
            "\n",
            "The word **strawberry** contains **3** instances of the letter **'r'**. \n",
            "\n",
            "Here's the breakdown:\n",
            "1. **S**  \n",
            "2. **T**  \n",
            "3. **R** (first occurrence)  \n",
            "4. **A**  \n",
            "5. **W**  \n",
            "6. **B**  \n",
            "7. **E**  \n",
            "8. **R** (second occurrence)  \n",
            "9. **R** (third occurrence)  \n",
            "10. **Y**  \n",
            "\n",
            "**Answer:** 3."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic Tooling\n",
        "\n",
        "---\n",
        "\n",
        "solve problems by taking action and intelligently uses external tools - starting with web search and code execution - alongside the powerful Llama 4 models and Llama 3.3 70b model\n",
        "\n",
        "---\n",
        "\n",
        "Both systems support the following tools:\n",
        "\n",
        "\n",
        "\n",
        "*   Web Search via [Tavily](https://www.tavily.com/)\n",
        "*   Code Execution via [E2B](https://e2b.dev/) (only Python is currently supported)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bRWk1-quKxay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the current weather in Tokyo?\",\n",
        "        }\n",
        "    ],\n",
        "    # Change model to compound-beta to use agentic tooling\n",
        "    # model: \"llama-3.3-70b-versatile\",\n",
        "    model=\"compound-beta\",\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n",
        "# Print all tool calls\n",
        "# print(completion.choices[0].message.executed_tools)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSXK4PVQKymG",
        "outputId": "4cdab328-52d2-49a0-d10d-2e5bc2425f0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current weather in Tokyo is partly cloudy with a temperature of 29¬∞C (84.2¬∞F), wind speed of 28.4 kph (17.7 mph) from the SSW, humidity of 66%, and pressure of 1008.0 mb (29.77 in).\n"
          ]
        }
      ]
    }
  ]
}